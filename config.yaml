# Semantic Chaos Bench Configuration

# Perturbation levels to test
epsilon_levels:
  - 0.01  # minimal perturbation
  - 0.05  # small perturbation
  - 0.10  # moderate perturbation
  - 0.20  # large perturbation

# Models to benchmark
models:
  openai:
    - gpt-4o-mini
  anthropic:
    - claude-haiku-4-5
  google:
    - gemini-2.5-flash
  replicate:
    - meta/meta-llama-3-8b-instruct
  together:
    - meta-llama/Meta-Llama-3-8B-Instruct-Lite

# Prompt categories
categories:
  - factual
  - creative
  - reasoning
  - code

# Generation parameters
generation:
  temperatures: [0.0, 0.7, 1.0]
  max_tokens: 500
  multi_step_count: 5

# Experiment settings
experiment:
  pairs_per_category: 100
  random_seed: 42
  cache_responses: true

# Embedding settings
embedding_model: "all-MiniLM-L6-v2"  # Fast, good quality
# Alternative: "all-mpnet-base-v2"  # Higher quality, slower

# Paraphrase generation settings
paraphrase_model: "gemini-2.5-flash"  # Model to use for generating paraphrases
n_paraphrases: 100  # Number of paraphrases to generate per base prompt
n_pairs_per_prompt: 10  # Number of pairs to generate per base prompt
tolerance: 0.01  # Tolerance for epsilon matching (epsilon Â± tolerance)

# API settings
api:
  # Retry configuration
  max_retries: 3
  retry_delay: 1.0  # seconds, base delay for exponential backoff
  timeout: 30  # seconds
  
  # Rate limiting (provider-specific)
  rate_limits:
    openai: 0.5      # seconds between requests
    anthropic: 0.5   # seconds between requests
    google: 1.0      # seconds between requests (more conservative)
    replicate: 1.0   # seconds between requests (can be slow)
    together: 0.5    # seconds between requests
  
  # Caching configuration
  cache:
    enabled: true
    directory: "data/cache"

# Output settings
output:
  results_dir: "experiments/results"
  cache_dir: "data/cache"
  plot_format: "png"
  plot_dpi: 300

